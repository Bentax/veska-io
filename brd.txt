
--Проверка состояния и устранение ошибок
--Проверка текущих задач в ClickHouse:

SELECT *
FROM system.mutations
WHERE table = 'aggregates_1h';
--Это поможет вам увидеть, есть ли незавершенные изменения (мутаторы), которые блокируют выполнение новых команд.

--Проверка реплик:

SELECT *
FROM system.replication_queue
WHERE table = 'aggregates_1h';
---Это покажет, есть ли задачи репликации, которые нужно завершить.

SYSTEM SYNC REPLICA aggregates_1h;
###################
ALTER TABLE aggregates_1h DROP INDEX idx_agg_timestamp;
ALTER TABLE aggregates_1h DROP INDEX idx_market;
ALTER TABLE aggregates_1h DROP INDEX idx_exchange;
###################
5/08/24
ALTER TABLE aggregates_1h
ADD INDEX idx_agg_timestamp (agg_timestamp) TYPE minmax GRANULARITY 1;

ALTER TABLE aggregates_1h
ADD INDEX idx_market (market) TYPE set(100) GRANULARITY 1;

ALTER TABLE aggregates_1h
ADD INDEX idx_exchange (exchange) TYPE set(100) GRANULARITY 1;

###################
1/08/24
п2 - Вроде ты уже делал такое уже. Пример как я сделал можешь глянуть например в price diff в Basic Views v2. Я всегда сначала подготавливаю данные из которых буду лепить конечный рещультат. Делаю "WITH" и называю обычно "exchange_data_frame". Потом уже из жтих данных строю основную выюборку
31/07/24
1) Добавь плз еще два графика "Buy / Sell Ratio by Tokens (Taker)" и "Buy / Sell Ratio by USD (Taker)"
2) Выбирая из таблицы aggregates ты не избавился от дубликатов. Это нажо жедлать всегда. так как мы будем перезапускать выборки и вставлять аггрегаты "патчами". Дубликаты будут 100%
3) Ты приводишь строки к нужной гранулярности (создаешь поле grouped_datetime). Потом ты берешь AVG от значения. Надо не AVG а SUMM. Так как если мы, нпармер, выбрали гранулярность - день, то чтобы узнать объем за день, нам надо просуммировать значения, а не взять AVG. (Конечно при условии что там нет дубликатов)
4) Добавь плз поддержку фильтра window_size
Это оконная фукнция которая считает нужную метрику за окно (достань из любого графика на Basic Views). Это похоже на гранулярность. Но гранулярность отвечает за "разрешение" (сколько строк данных мы видим), а window_size - отвечает за "сглаживание"

#####################
OVER (PARTITION BY name) - https://habr.com/ru/articles/664000/

Текущий статус
В данный момент мы совершаем переезд с инфраструктуры AWS + развернутой
самостоятельно БД -> в Google Cloud + Clickhouse Cloud.
Создаем таблицы заново, настраиваем потоки и т.д.
Сейчас в новом Clickhouse только одна таллица в схеме Private
Что такое Private
В БД есть две схемы public и private
Public - это подмножество от Private. Тоесть Private содержит в себе все таблицы, которые есть в Public + все данные, что есть в Public. 
Но в Private есть данные которых нет в Public, и могут быть таблицы которых нет в Public
Часть исследований мы делаем для себя или кого-то еще, и не хотим раньше времени это показывать. 
Поэтому, мы вцелом физически разделяем эти два направления Public/Private. 
Храним код в разных местах, дублируем данные и коннекторы
Немного о специфике работы с Clickhouse
Кликхаус специализируется на быстрой вставке данных и быстрой "онлайн" аналитке. 
Это подразумевает под собой некоторые tradeoffs. Например, здесь нет привычных constraints по ключу. 
Тоесть, нельзя указать первичный ключ и ожидать, что в данных не будет дубликатов. 
ДУБЛИКАТЫ ВСЕГДА ЕСТЬ. Даже если ты используешь какой-нибудь движок таблицы ReplacingMergeTree или AggregatingMergeTree. 
В конечной своей выборке надо обязательно делать предварительный GROUP BY подготавливая данные.

Задачи
Сейчас в схеме Private есть таблица futures_trades_stream.
Нужно создать таблицу exchanges_events_1h
Структура таблицы такая-же, как в у message Event описанного в файле protobuff:
https://github.com/veska-io/streams-proto/blob/main/proto/main/exchanges_events.proto
+также надо добавить туда поле updated_timestamp и установить ему значение по
дефолту равное текущему значению UnixTimestamp в МИЛЛИСЕКУНДАХ. Тоесть число должно быть что-то вроде 1257894000000
TTL таблицы установить в 40 дней
Движок можно сделать обычный MergeTree

Нужно написать запрос, который из данных таблицы futures_trades_stream будет создавать события в таблице exchanges_events_1h
Где event может быть: price | volume | trades | liquidations
Тоесть мы должны породить 4 записи на каждый час, где в каждой из записей заполнены
(сагрегированны) поля с префиксом ивента. А остальные поля выставлены в NULL.

Далее нужно создать таблицу aggregates_1h
Структура ее точно такая-же как и exchange_events, но без поля event и поле
event_timestamp переименовано в agg_timestamp 
TTL тут не нужен
Добавить партиционирование по Году
Установить движок ReplacingMergeTree
Подобрать первичный ключ, который будет явно определять уникальность часового аггрегата
Сделать запрос в таблицу exchanges_events_1h, который будет аггрегировать ивенты за
1h и собирать цельную строку без NULL-ов и записывать уже чистый аггрегат в aggregates

Вот пример создания талицы. Тут нет установления TTL и есть ненужный для exchanges_events_1h PARTITION BY

CREATE TABLE IF NOT EXISTS futures_trades_stream
(
updated_at DateTime DEFAULT now(),
trade_timestamp UInt64 NOT NULL,
created_at_height Nullable(UInt64) DEFAULT NULL,
trade_id Nullable(String) DEFAULT NULL,
agg_id Nullable(String) DEFAULT NULL,
start_trade_id Nullable(String) DEFAULT NULL,
end_trade_id Nullable(String) DEFAULT NULL,
exchange String NOT NULL,
market String NOT NULL,
base String NOT NULL,
quot String NOT NULL,
side String NOT NULL,
size Float64 NOT NULL,
price Float64 NOT NULL,
trade_type Nullable(String) DEFAULT NULL,
is_buyer_maker Nullable(bool) DEFAULT NULL
) ENGINE = MergeTree
PARTITION BY toYear(fromUnixTimestamp(trade_timestamp))
ORDER BY (trade_timestamp, exchange, market)

Тебе нужно сначала на основе futures_trades_stream создать ивенты в exchange_events_1h
Ликвидации - это трейды у которых в type = "LIQUIDATED"
Обрати внимание что в тадице exchanges_events должно получиться 4 записи по одному часу с event-ами price / volume / trades / liquidations
Тоже обрати внимание что volume_base_sell_taker - это side BUY. Это не ошибка Так как в трейде всегда есть тейкре и мейкер. Тут мы считаем тейкера

price_open // первая цена в часовом интервале
price_close // последняя цена в часовом интервале
price_high // максимальная цена в часовом интервале
price_low // минимальная цена в часовом интервале
volume_quot // объем в токенах
volume_base // объем в USD
volume_base_sell_taker // объем трейдов с SIDE = BUY и TYPE = LIMIT
volume_base_buy_taker // объем трейдов с SIDE = SELL и TYPE = LIMIT
oi_open // пока нет
trades_count // количество трейдов в интервале
liquidations_sell_count // количество трейдов с SIDE = SELL и TYPE = LIQUIDATION
liquidations_buy_count // количество трейдов с SIDE = BUY и TYPE = LIQUIDATION 
liquidations_sell_base_volume // объем в токенах трейдов с SIDE = SELL и TYPE = LIQUIDATION 
liquidations_buy_base_volume // объем в токенах трейдов с SIDE = BUY и TYPE = LIQUIDATION
liquidations_sell_quot_volume // объем в USD
liquidations_buy_quot_volume // объем в USD

https://github.com/veska-io/dashboards-grpc/blob/main/src/services/dashboards/sql/price_diff.sql
посмотри на код этого запроса

А именно - на секцию exchange_data_frame

Твой запрос по аггрегациям посчитает неправильно. 
Так как в базе будут дубликаты 100%. 
Поэтому перед тем как делать аггрегации нужно сгруппировать все по "первичному ключу", а так-же отсортировать по updated_timestamp DESC. 
Сделать для этой первичной аггрегации groupArray значений и достать оттуда первое (самое новое, так как мы отсортировали по дате вставки). 
И вот когда данные у тебя уже гарантированно в единичном экземпляре - тогда делай основной запрос выборкой из этой CTE

################################ 15.07.24 ############################################

Volume
Поля которые есть в таблице
Объем в токене (например в BTC)
volume_base
Объем в USD
volume_quot
Объем токенов купленных Тейкером (исполнился SELL LIMIT ордер)
volume_base_buy_taker
Объем USD полученных Тейкером (исполнился BUY LIMIT ордер)
volume_quot_buy_taker
Объем токенов проданных Тейкером (исполнился BUY LIMIT ордер)
volume_base_sell_taker
Объем USD отданных Тейкером (исполнился SELL LIMIT ордер)
volume_quot_sell_taker
Reports
Volume USD
volume_quot разбитый по маркетам
Buy / Sell Ratio by Tokens (Taker)
(VBBT - VBST) / MAX (VBBT, VBST)
Назовем это B
Должны получить график который будет лежать в пределах от 1 до -1.
Что-то вроде такого
Buy / Sell Ratio by USD (Taker)
(VQBT - VQST) / MAX (VQBT, VQST)
Назовем это Q
Тоже самое как и впрошлом варианте
Tokens / USD Difference
|B| - |Q|
Должны получить график где все линии близки к 0

Andrei Pokhila, [13.07.2024 19:08]
Свечи от бинанса: binance_futures_klines_1h

Пустая таблица ивентов: futures_exchanges_events_1h

Нужно перелить данные с таьблицы свечей в ивенты
А потом создать таблицу аггрегатов и заполнить ее
Пригласил теюя в новый клауд клика

Обрати внимание, что я немного поменял таблицу ивентов (название полей). Тоесть таблица аггрегатов тоде будет выглядеть подругому

После того как данные будут готовы. То надо сделать несколько дэщбордов. Какие - я описал в пдф

Вот твоя папка, где можешь созщдавать дэшборды
http://54.236.30.177:3000/dashboards/f/edb93c1a-fa9f-4a34-ba1d-6d96b9b57be6/rusakkurat

Вот дэшборд который надо наполнить:
http://54.236.30.177:3000/d/c4aac7d1-3615-421c-a8a8-763bb5b8c9a0/volume?orgId=1

Andrei Pokhila, [13.07.2024 19:08]
У тебя есть доступ ко всем дэшбордам. Ничего в них не меняй
Вот тут ,http://54.236.30.177:3000/d/basic_views/basic-views-new?orgId=1
можешь посмотреть как сделаны графики. По сути можешь просто скопировать нужную панель через контекстное меню, и вставить в свой лэшборд. А потом поменять выборку

В любом случае, как дойдешь до этого момента, то можем сделать 20мин созвон и я покажу как делать графики. Там ничего сложного нет

Вот как получить нужные поля в exchange_events из тыблицы свеч бинанса:
volume_base = volume
volume_quot = quot_asset_volume
volume_base_buy_taker = taker_buy_base_asset_volume
volume_quot_buy_taker = taker_buy_quot_asset_volume
volume_base_sell_taker = volume - taker_buy_base_asset_volume
volume_quot_sell_taker = quot_asset_volume - taker_buy_quot_asset_volume
